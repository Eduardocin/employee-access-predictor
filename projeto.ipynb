{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eduardocin/employee-access-predictor/blob/pedroza-refactor/projeto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a374a36",
      "metadata": {
        "id": "1a374a36"
      },
      "source": [
        "# Análise Preditiva de Acesso a Recursos (Amazon) - Equipe 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d83bc5d",
      "metadata": {
        "id": "3d83bc5d"
      },
      "source": [
        "## **Estrutura do Notebook**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **0.0 Configurações Iniciais** - Importação de bibliotecas e configurações\n",
        "- **1.0 Carregamento e Visualização dos Dados** - Carregamento e primeira visualização\n",
        "- **2.0 Verificação da Qualidade dos Dados** - Valores ausentes, duplicados e tipos de dados\n",
        "- **3.0 Análise Exploratória dos Dados (EDA)** - Estatísticas, visualizações e padrões\n",
        "- **4.0 Pré-processamento** - Conclusões e próximos passos\n",
        "- **5.0. Seleção de Modelos** -\n",
        "- **6.0 Busca de Hiperparâmetros** -"
      ],
      "metadata": {
        "id": "iE94Q_j3Mmk_"
      },
      "id": "iE94Q_j3Mmk_"
    },
    {
      "cell_type": "markdown",
      "id": "3f23e34c",
      "metadata": {
        "id": "3f23e34c"
      },
      "source": [
        "## 0.0 Configurações Iniciais"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ee83217",
      "metadata": {
        "id": "5ee83217"
      },
      "source": [
        "### 0.1 Importação de Bibliotecas e Configurações"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install category_encoders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuUJZyZJT1E3",
        "outputId": "f287d247-ec17-4713-8f1f-ad9b3967d895"
      },
      "id": "GuUJZyZJT1E3",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.8.1-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (2.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (1.16.0)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (0.14.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category_encoders) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.6.0->category_encoders) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.6.0->category_encoders) (3.6.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.9.0->category_encoders) (25.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->category_encoders) (1.17.0)\n",
            "Downloading category_encoders-2.8.1-py3-none-any.whl (85 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/85.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: category_encoders\n",
            "Successfully installed category_encoders-2.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6fe87881",
      "metadata": {
        "id": "6fe87881"
      },
      "outputs": [],
      "source": [
        "# Importação das bibliotecas necessárias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import randint, loguniform\n",
        "from random import seed, randrange\n",
        "from math import sqrt\n",
        "import time\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_validate, RandomizedSearchCV\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import make_scorer, precision_score, recall_score\n",
        "from category_encoders import TargetEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Modelos\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configurações para visualização\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "797925a4",
      "metadata": {
        "id": "797925a4"
      },
      "source": [
        "### 0.2 Definição de Funções Auxiliares"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42db45f7",
      "metadata": {
        "id": "42db45f7"
      },
      "outputs": [],
      "source": [
        "seed(1)\n",
        "\n",
        "# Convert string column to float\n",
        "def str_column_to_float(dataset, column):\n",
        "\tfor row in dataset:\n",
        "\t\trow[column] = float(row[column].strip())\n",
        "\n",
        "# Convert string column to integer\n",
        "def str_column_to_int(dataset, column):\n",
        "\tclass_values = [row[column] for row in dataset]\n",
        "\tunique = set(class_values)\n",
        "\tlookup = dict()\n",
        "\tfor i, value in enumerate(unique):\n",
        "\t\tlookup[value] = i\n",
        "\tfor row in dataset:\n",
        "\t\trow[column] = lookup[row[column]]\n",
        "\treturn lookup\n",
        "\n",
        "# Split a dataset into k folds\n",
        "def cross_validation_split(dataset, n_folds):\n",
        "\tdataset_split = list()\n",
        "\tdataset_copy = list(dataset)\n",
        "\tfold_size = int(len(dataset) / n_folds)\n",
        "\tfor i in range(n_folds):\n",
        "\t\tfold = list()\n",
        "\t\twhile len(fold) < fold_size:\n",
        "\t\t\tindex = randrange(len(dataset_copy))\n",
        "\t\t\tfold.append(dataset_copy.pop(index))\n",
        "\t\tdataset_split.append(fold)\n",
        "\treturn dataset_split\n",
        "\n",
        "# Calculate accuracy percentage\n",
        "def accuracy_metric(actual, predicted):\n",
        "\tcorrect = 0\n",
        "\tfor i in range(len(actual)):\n",
        "\t\tif actual[i] == predicted[i]:\n",
        "\t\t\tcorrect += 1\n",
        "\treturn correct / float(len(actual)) * 100.0\n",
        "\n",
        "# Evaluate an algorithm using a cross validation split\n",
        "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
        "\tfolds = cross_validation_split(dataset, n_folds)\n",
        "\tscores = list()\n",
        "\tfor fold in folds:\n",
        "\t\ttrain_set = list(folds)\n",
        "\t\ttrain_set.remove(fold)\n",
        "\t\ttrain_set = sum(train_set, [])\n",
        "\t\ttest_set = list()\n",
        "\t\tfor row in fold:\n",
        "\t\t\trow_copy = list(row)\n",
        "\t\t\ttest_set.append(row_copy)\n",
        "\t\t\trow_copy[-1] = None\n",
        "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
        "\t\tactual = [row[-1] for row in fold]\n",
        "\t\taccuracy = accuracy_metric(actual, predicted)\n",
        "\t\tscores.append(accuracy)\n",
        "\treturn scores\n",
        "\n",
        "# calculate the Euclidean distance between two vectors\n",
        "def euclidean_distance(row1, row2):\n",
        "\tdistance = 0.0\n",
        "\tfor i in range(len(row1)-1):\n",
        "\t\tdistance += (row1[i] - row2[i])**2\n",
        "\treturn sqrt(distance)\n",
        "\n",
        "# Locate the best matching unit\n",
        "def get_best_matching_unit(codebooks, test_row):\n",
        "\tdistances = list()\n",
        "\tfor codebook in codebooks:\n",
        "\t\tdist = euclidean_distance(codebook, test_row)\n",
        "\t\tdistances.append((codebook, dist))\n",
        "\tdistances.sort(key=lambda tup: tup[1])\n",
        "\treturn distances[0][0]\n",
        "\n",
        "# Make a prediction with codebook vectors\n",
        "def predict(codebooks, test_row):\n",
        "\tbmu = get_best_matching_unit(codebooks, test_row)\n",
        "\treturn bmu[-1]\n",
        "\n",
        "# Create a random codebook vector\n",
        "def random_codebook(train):\n",
        "\tn_records = len(train)\n",
        "\tn_features = len(train[0])\n",
        "\tcodebook = [train[randrange(n_records)][i] for i in range(n_features)]\n",
        "\treturn codebook\n",
        "\n",
        "# Train a set of codebook vectors\n",
        "def train_codebooks(train, n_codebooks, lrate, epochs):\n",
        "\tcodebooks = [random_codebook(train) for i in range(n_codebooks)]\n",
        "\tfor epoch in range(epochs):\n",
        "\t\trate = lrate * (1.0-(epoch/float(epochs)))\n",
        "\t\tfor row in train:\n",
        "\t\t\tbmu = get_best_matching_unit(codebooks, row)\n",
        "\t\t\tfor i in range(len(row)-1):\n",
        "\t\t\t\terror = row[i] - bmu[i]\n",
        "\t\t\t\tif bmu[-1] == row[-1]:\n",
        "\t\t\t\t\tbmu[i] += rate * error\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tbmu[i] -= rate * error\n",
        "\treturn codebooks\n",
        "\n",
        "# LVQ Algorithm\n",
        "def learning_vector_quantization(train, test, n_codebooks, lrate, epochs):\n",
        "\tcodebooks = train_codebooks(train, n_codebooks, lrate, epochs)\n",
        "\tpredictions = list()\n",
        "\tfor row in test:\n",
        "\t\toutput = predict(codebooks, row)\n",
        "\t\tpredictions.append(output)\n",
        "\treturn(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.3 Implementação LVQ"
      ],
      "metadata": {
        "id": "RUMbn1_vTfzu"
      },
      "id": "RUMbn1_vTfzu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e62db239",
      "metadata": {
        "id": "e62db239"
      },
      "outputs": [],
      "source": [
        "# --- Classe Adaptadora para o LVQ ---\n",
        "class LVQClassifier(BaseEstimator, ClassifierMixin):\n",
        "    # O construtor\n",
        "    def __init__(self, l_rate=0.3, n_epochs=50, n_codebooks=10):\n",
        "        self.l_rate = l_rate\n",
        "        self.n_epochs = n_epochs\n",
        "        self.n_codebooks = n_codebooks\n",
        "        self.codebooks_ = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Este método agora lida corretamente com DataFrames, Series ou arrays NumPy.\n",
        "        \"\"\"\n",
        "        # Converte X para NumPy, caso seja um DataFrame do pandas\n",
        "        X_np = X if isinstance(X, np.ndarray) else X.values\n",
        "\n",
        "        # Converte y para NumPy e garante o formato (shape)\n",
        "        y_np = np.array(y).reshape(-1, 1)\n",
        "\n",
        "        # Junta os dados para o formato que a função de treino original espera\n",
        "        training_data = np.hstack([X_np, y_np]).tolist()\n",
        "\n",
        "        # Chama a sua função de treino original\n",
        "        self.codebooks_ = train_codebooks(training_data, self.n_codebooks, self.l_rate, self.n_epochs)\n",
        "        return self\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "            \"\"\"\n",
        "            Realiza as predições, também garantindo que X seja um array NumPy.\n",
        "            \"\"\"\n",
        "            if self.codebooks_ is None:\n",
        "                raise RuntimeError(\"Você precisa treinar o classificador primeiro com o método .fit()\")\n",
        "\n",
        "            # Converte X para NumPy, caso seja um DataFrame do pandas\n",
        "            X_np = X if isinstance(X, np.ndarray) else X.values\n",
        "\n",
        "            # Chama a sua função de predição original para cada linha\n",
        "            y_pred = [lvq_predict(self.codebooks_, row) for row in X_np]\n",
        "            return np.array(y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CRISP-DM Fase 2: Exploração dos Dados"
      ],
      "metadata": {
        "id": "VC2VtqnwMGnG"
      },
      "id": "VC2VtqnwMGnG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objetivo\n",
        "\n"
      ],
      "metadata": {
        "id": "WZdzOfYmMIxX"
      },
      "id": "WZdzOfYmMIxX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparar e transformar os dados de acesso a recursos da Amazon para modelagem preditiva, incluindo limpeza, transformação de variáveis categóricas, tratamento de desbalanceamento e seleção de features relevantes para determinar se um funcionário deve ter acesso a determinados recursos."
      ],
      "metadata": {
        "id": "UacHzHr5MN9G"
      },
      "id": "UacHzHr5MN9G"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Informações do Dataset\n"
      ],
      "metadata": {
        "id": "IuXebXn5MKav"
      },
      "id": "IuXebXn5MKav"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Fonte**: Amazon Employee Access Challenge\n",
        "- **Problema**: Classificação binária (0 = Acesso Negado, 1 = Acesso Permitido)\n",
        "- **Dados de Treinamento**: 32.769 registros\n",
        "- **Abordagem**: Análise exploratória apenas nos dados de treino para evitar data leakage"
      ],
      "metadata": {
        "id": "sVDQhdaxMPZv"
      },
      "id": "sVDQhdaxMPZv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e0e880b",
      "metadata": {
        "id": "5e0e880b"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "# --- Avaliando o LVQ com as ferramentas do Scikit-learn ---\n",
        "\n",
        "# 1. Instanciar o nosso novo classificador LVQ\n",
        "# Podemos ajustar os parâmetros aqui, se quisermos.\n",
        "lvq_model = LVQClassifier(l_rate=0.2, n_epochs=100, n_codebooks=20)\n",
        "\n",
        "# 2. Definir a estratégia de validação cruzada\n",
        "# Usaremos StratifiedKFold para garantir que a proporção das classes seja mantida em cada fold.\n",
        "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# 3. Executar a validação cruzada\n",
        "# O cross_val_score vai treinar e avaliar nosso modelo 5 vezes, usando a estratégia definida.\n",
        "# Nota: os dados X_train e y_train devem ser arrays do NumPy. Se forem DataFrames do Pandas, converta com .values\n",
        "X_train_np = X_train.values if hasattr(X_train, 'values') else X_train\n",
        "y_train_np = y_train.values if hasattr(y_train, 'values') else y_train\n",
        "\n",
        "scores = cross_val_score(lvq_model, X_train_np, y_train_np, cv=cv_strategy, scoring='accuracy')\n",
        "\n",
        "# 4. Exibir os resultados\n",
        "print(f\"Acurácia do LVQ em cada um dos 5 folds: {scores}\")\n",
        "print(f\"Acurácia Média do LVQ: {scores.mean():.4f} (+/- {scores.std():.4f})\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07661c4c",
      "metadata": {
        "id": "07661c4c"
      },
      "source": [
        "## 1.0 Carregamento e Visualização dos Dados\n",
        "\n",
        "Nesta seção, realizamos o carregamento dos dados de treinamento e fazemos uma primeira visualização da estrutura do dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff82053c",
      "metadata": {
        "id": "ff82053c"
      },
      "outputs": [],
      "source": [
        "# Carregamento dos dados de treinamento\n",
        "train_df = pd.read_csv('data/train.csv')\n",
        "\n",
        "# Separar features e target\n",
        "X = train_df.drop(columns=['ACTION'])\n",
        "y = train_df['ACTION']\n",
        "\n",
        "# (80% treino, 20% teste)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.20,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cee63e2",
      "metadata": {
        "id": "6cee63e2"
      },
      "outputs": [],
      "source": [
        "X_train.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5310092",
      "metadata": {
        "id": "f5310092"
      },
      "source": [
        "### 1.1 Carregamento dos Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "327bb433",
      "metadata": {
        "id": "327bb433"
      },
      "outputs": [],
      "source": [
        "# Visualização do head de treino\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a04af8c",
      "metadata": {
        "id": "3a04af8c"
      },
      "source": [
        "## 2.0 Verificação da Qualidade dos Dados\n",
        "\n",
        "Antes de qualquer análise exploratória, é fundamental verificar a qualidade dos dados para identificar possíveis problemas como valores ausentes, duplicados ou inconsistências que possam afetar a análise."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b158b968",
      "metadata": {
        "id": "b158b968"
      },
      "source": [
        "### 2.1 Estrutura Geral do Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de44cb3a",
      "metadata": {
        "id": "de44cb3a"
      },
      "outputs": [],
      "source": [
        "# Análise da estrutura dos dados de treino após divisão\n",
        "print(\"=== INFORMAÇÕES GERAIS ===\")\n",
        "print(f\"Dimensões: {X_train.shape}\")\n",
        "print(f\"Tipos de dados:\\n{X_train.dtypes}\")\n",
        "print(f\"Memória utilizada: {X_train.memory_usage().sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "# Separar variáveis preditoras e target\n",
        "features = list(X_train.columns)\n",
        "target = 'ACTION'\n",
        "\n",
        "print(f\"\\nVariável target: {target}\")\n",
        "print(f\"Variáveis preditoras: {features}\")\n",
        "print(f\"Número de variáveis preditoras: {len(X_train.columns)}\")\n",
        "print(f\"Tamanho de y_train: {y_train.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14cb79d1",
      "metadata": {
        "id": "14cb79d1"
      },
      "source": [
        "### 2.2 Verificação de Valores Ausentes e Duplicados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfe6530c",
      "metadata": {
        "id": "dfe6530c"
      },
      "outputs": [],
      "source": [
        "# Verificação de valores ausentes\n",
        "print(\"\\n=== VALORES AUSENTES ===\")\n",
        "missing_values = X_train.isnull().sum()\n",
        "total_missing = missing_values.sum()\n",
        "\n",
        "if total_missing == 0:\n",
        "    print(\"Nenhum valor ausente encontrado!\")\n",
        "else:\n",
        "    print(f\"Total de valores ausentes: {total_missing}\")\n",
        "    print(\"\\nPor variável:\")\n",
        "    for col, missing in missing_values.items():\n",
        "        if missing > 0:\n",
        "            percentage = (missing / len(train_df)) * 100\n",
        "            print(f\"  {col}: {missing} ({percentage:.2f}%)\")\n",
        "\n",
        "# Verificação de valores nulos/duplicados\n",
        "print(\"\\n=== VALORES DUPLICADOS ===\")\n",
        "duplicates = X_train.duplicated().sum()\n",
        "if duplicates == 0:\n",
        "    print(\"Nenhum registro duplicado encontrado!\")\n",
        "else:\n",
        "    print(f\"{duplicates} registros duplicados encontrados\")\n",
        "\n",
        "# Resumo geral\n",
        "print(f\"\\n=== RESUMO GERAL ===\")\n",
        "print(f\"Total de registros: {len(X_train):,}\")\n",
        "print(f\"Total de variáveis: {len(X_train.columns)}\")\n",
        "print(f\"Valores ausentes: {total_missing}\")\n",
        "print(f\"Registros duplicados: {duplicates}\")\n",
        "\n",
        "if total_missing == 0 and duplicates == 0:\n",
        "    print(\"\\nDados prontos para análise!\")\n",
        "else:\n",
        "    print(\"\\nDados requerem limpeza antes da modelagem\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cb5102b",
      "metadata": {
        "id": "2cb5102b"
      },
      "source": [
        "### 2.3 Análise de Tipos de Dados e contagem de valores únicos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0934c498",
      "metadata": {
        "id": "0934c498"
      },
      "outputs": [],
      "source": [
        "# Verificação dos tipos de dados e valores únicos\n",
        "print(\"\\n=== TIPOS DE DADOS E VALORES ÚNICOS ===\")\n",
        "\n",
        "# Informações dos tipos de dados\n",
        "print(\"Tipos de dados:\")\n",
        "for col in X_train.columns:\n",
        "    dtype = X_train[col].dtype\n",
        "    unique_count = X_train[col].nunique()\n",
        "    print(f\"  {col}: {dtype} ({unique_count} valores únicos)\")\n",
        "\n",
        "# Verificação específica da variável target\n",
        "print(f\"\\n=== VARIÁVEL TARGET (ACTION) ===\")\n",
        "target_counts = y_train.value_counts()\n",
        "print(f\"Distribuição da variável target:\")\n",
        "for value, count in target_counts.items():\n",
        "    percentage = (count / len(X_train)) * 100\n",
        "    print(f\"  {value}: {count} ({percentage:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dafcf637",
      "metadata": {
        "id": "dafcf637"
      },
      "source": [
        "## 3.0 Análise Exploratória dos Dados\n",
        "Agora que verificamos a qualidade dos dados, vamos realizar uma análise exploratória detalhada das características do dataset, incluindo análise estatística, visualizações e identificação de padrões."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a35d5645",
      "metadata": {
        "id": "a35d5645"
      },
      "source": [
        "### 3.1 Estatísticas Descritivas e Análise da Variável Target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d503d10",
      "metadata": {
        "id": "5d503d10"
      },
      "outputs": [],
      "source": [
        "# Análise estatística básica das variáveis numéricas\n",
        "print(\"\\n=== ESTATÍSTICAS DESCRITIVAS - DATASET DE TREINAMENTO ===\")\n",
        "print(X_train.describe())\n",
        "\n",
        "# Análise da variável target\n",
        "print(\"\\n=== ANÁLISE DA VARIÁVEL TARGET (ACTION) ===\")\n",
        "target_counts = y_train.value_counts()\n",
        "target_props = y_train.value_counts(normalize=True)\n",
        "\n",
        "print(f\"Distribuição da variável target:\")\n",
        "for i, (count, prop) in enumerate(zip(target_counts, target_props)):\n",
        "    print(f\"  Classe {target_counts.index[i]}: {count} ({prop:.2%})\")\n",
        "\n",
        "print(f\"\\nBalanceamento das classes:\")\n",
        "print(f\"  Diferença: {abs(target_props.iloc[0] - target_props.iloc[1]):.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61fee4a2",
      "metadata": {
        "id": "61fee4a2"
      },
      "source": [
        "### 3.2 Análise Individual da proporção da variável target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09fe3b05",
      "metadata": {
        "id": "09fe3b05"
      },
      "outputs": [],
      "source": [
        "# Análise univariada das variáveis categóricas\n",
        "categorical_vars = [col for col in X_train.columns]\n",
        "\n",
        "# Visualização da distribuição da variável target\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Definir cores intuitivas: vermelho para negado, verde para permitido\n",
        "colors = [\"#3ACE78\", \"#CE594C\"]  # Verde e Vermelho (invertido para barras)\n",
        "\n",
        "# Gráfico de barras\n",
        "target_counts = y_train.value_counts()\n",
        "bars = axes[0].bar(target_counts.index, target_counts.values, color=colors)\n",
        "axes[0].set_title('Distribuição da Variável Target', fontsize=12, fontweight='bold')\n",
        "axes[0].set_xlabel('Classe')\n",
        "axes[0].set_ylabel('Frequência')\n",
        "axes[0].set_xticks([0, 1])\n",
        "axes[0].set_xticklabels(['Acesso Negado', 'Acesso Permitido'])\n",
        "\n",
        "# Adicionar valores nas barras\n",
        "for bar, count in zip(bars, target_counts.values):\n",
        "    axes[0].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 200,\n",
        "                f'{count}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Gráfico de pizza\n",
        "axes[1].pie(target_counts.values,\n",
        "            labels=['Acesso Permitido','Acesso Negado'],\n",
        "            autopct='%1.1f%%', colors=colors,\n",
        "            startangle=90)\n",
        "axes[1].set_title('Proporção das Classes', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcffac14",
      "metadata": {
        "id": "dcffac14"
      },
      "source": [
        "### 3.3 Análise Número de Categorias por Feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef884dfd",
      "metadata": {
        "id": "ef884dfd"
      },
      "outputs": [],
      "source": [
        "# Calcular o número de categorias únicas para cada feature preditiva\n",
        "features = [col for col in X_train.columns]\n",
        "cardinality_data = []\n",
        "\n",
        "for feature in features:\n",
        "    unique_count = X_train[feature].nunique()\n",
        "    cardinality_data.append({'Feature': feature, 'Categorias_Unicas': unique_count})\n",
        "\n",
        "# DataFrame para facilitar a visualização\n",
        "cardinality_df = pd.DataFrame(cardinality_data)\n",
        "cardinality_df = cardinality_df.sort_values('Categorias_Unicas', ascending=False)\n",
        "\n",
        "#  gráfico de barras\n",
        "plt.figure(figsize=(14, 8))\n",
        "bars = plt.bar(range(len(cardinality_df)), cardinality_df['Categorias_Unicas'],\n",
        "               color='skyblue', edgecolor='darkblue', linewidth=1.2)\n",
        "\n",
        "# Personalizar o gráfico\n",
        "plt.title('Número de Categorias Distintas por Feature', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.xlabel('Features', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Número de Categorias Únicas', fontsize=12, fontweight='bold')\n",
        "plt.xticks(range(len(cardinality_df)), cardinality_df['Feature'], rotation=45, ha='right')\n",
        "\n",
        "# valores no topo de cada barra\n",
        "for i, bar in enumerate(bars):\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
        "             f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# grid para melhor visualização\n",
        "plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "# Ajustar layout\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# estatísticas resumidas\n",
        "print(f\"\\n=== ESTATÍSTICAS DE CARDINALIDADE ===\")\n",
        "print(f\"Feature com maior cardinalidade: {cardinality_df.iloc[0]['Feature']} ({cardinality_df.iloc[0]['Categorias_Unicas']} categorias)\")\n",
        "print(f\"Feature com menor cardinalidade: {cardinality_df.iloc[-1]['Feature']} ({cardinality_df.iloc[-1]['Categorias_Unicas']} categorias)\")\n",
        "print(f\"Cardinalidade média: {cardinality_df['Categorias_Unicas'].mean():.1f}\")\n",
        "print(f\"Cardinalidade mediana: {cardinality_df['Categorias_Unicas'].median():.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3721eeb4",
      "metadata": {
        "id": "3721eeb4"
      },
      "source": [
        "### 3.4 Análise bivariada - Relação entre variáveis preditoras e target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e05e24c",
      "metadata": {
        "id": "7e05e24c"
      },
      "outputs": [],
      "source": [
        "# Análise bivariada - Relação entre variáveis preditoras e target\n",
        "print(\"=== ANÁLISE BIVARIADA - VARIÁVEIS vs TARGET ===\")\n",
        "\n",
        "# Função para calcular taxa de aprovação por variável\n",
        "def analyze_approval_rate(df, var_name, top_n=10):\n",
        "    \"\"\"\n",
        "    Calcula a taxa de aprovação para os top N valores de uma variável\n",
        "    \"\"\"\n",
        "    approval_rates = df.groupby(var_name)['ACTION'].agg(['count', 'mean']).reset_index()\n",
        "    approval_rates.columns = [var_name, 'total_requests', 'approval_rate']\n",
        "    approval_rates = approval_rates.sort_values('total_requests', ascending=False).head(top_n)\n",
        "    return approval_rates\n",
        "\n",
        "# Juntar X_train e y_train para análise\n",
        "train_analysis_df = X_train.copy()\n",
        "train_analysis_df['ACTION'] = y_train\n",
        "\n",
        "variables_to_analyze = [var for var in categorical_vars if X_train[var].nunique() <= 500]\n",
        "\n",
        "for var in variables_to_analyze[:5]:  # Analisar apenas as primeiras 5 variáveis\n",
        "    print(f\"\\n=== {var} ===\")\n",
        "    approval_rates = analyze_approval_rate(train_analysis_df, var)\n",
        "\n",
        "    print(f\"Taxa de aprovação por {var} (Top 10 mais frequentes):\")\n",
        "    for _, row in approval_rates.iterrows():\n",
        "        print(f\"  {row[var]}: {row['approval_rate']:.1%} ({row['total_requests']} solicitações)\")\n",
        "\n",
        "    # Visualização\n",
        "    if len(approval_rates) > 2:\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        # Gráfico de barras para contagem\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.bar(range(len(approval_rates)), approval_rates['total_requests'])\n",
        "        plt.title(f'Frequência por {var}')\n",
        "        plt.xlabel('Categorias')\n",
        "        plt.ylabel('Número de Solicitações')\n",
        "        plt.xticks(range(len(approval_rates)), approval_rates[var], rotation=45)\n",
        "\n",
        "        # Gráfico de barras para taxa de aprovação\n",
        "        plt.subplot(1, 2, 2)\n",
        "        colors = ['red' if rate < 0.5 else 'green' for rate in approval_rates['approval_rate']]\n",
        "        plt.bar(range(len(approval_rates)), approval_rates['approval_rate'], color=colors)\n",
        "        plt.title(f'Taxa de Aprovação por {var}')\n",
        "        plt.xlabel('Categorias')\n",
        "        plt.ylabel('Taxa de Aprovação')\n",
        "        plt.xticks(range(len(approval_rates)), approval_rates[var], rotation=45)\n",
        "        plt.ylim(0, 1)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "910cdd2c",
      "metadata": {
        "id": "910cdd2c"
      },
      "source": [
        "### 3.5 Análise Matriz de Correlação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b18eae20",
      "metadata": {
        "id": "b18eae20"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "corr = train_df.corr()\n",
        "sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, cmap=sns.diverging_palette(220, 20, as_cmap=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12699464",
      "metadata": {
        "id": "12699464"
      },
      "source": [
        "### 3.6 Análise da Proporção Por Classe em Cada Feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f9d3b41",
      "metadata": {
        "id": "7f9d3b41"
      },
      "outputs": [],
      "source": [
        "def analyze_class_representation(df, col, target_col='ACTION', min_samples=10):\n",
        "    \"\"\"\n",
        "    Analisa a representação de classes dentro de cada categoria\n",
        "    \"\"\"\n",
        "    # Crosstab para ver distribuição de classes por categoria\n",
        "    crosstab = pd.crosstab(df[col], df[target_col], margins=True)\n",
        "\n",
        "    # Calcular proporções - só para categorias, não para o total\n",
        "    props = pd.crosstab(df[col], df[target_col], normalize='index') * 100\n",
        "\n",
        "    # Garantir que temos ambas as classes (0 e 1) no crosstab\n",
        "    if 0 not in crosstab.columns:\n",
        "        crosstab = crosstab.copy()\n",
        "        crosstab[0] = 0\n",
        "    if 1 not in crosstab.columns:\n",
        "        crosstab = crosstab.copy()\n",
        "        crosstab[1] = 0\n",
        "\n",
        "    # Garantir que temos ambas as classes (0 e 1) no props\n",
        "    if 0 not in props.columns:\n",
        "        props = props.copy()\n",
        "        props[0] = 0.0\n",
        "    if 1 not in props.columns:\n",
        "        props = props.copy()\n",
        "        props[1] = 0.0\n",
        "\n",
        "    # Reordenar colunas para garantir ordem consistente\n",
        "    crosstab = crosstab[[0, 1, 'All']]\n",
        "    props = props[[0, 1]]\n",
        "\n",
        "    # Remover a linha 'All' do crosstab e props\n",
        "    crosstab_categories = crosstab.drop('All', axis=0)\n",
        "    props_categories = props.drop('All', axis=0) if 'All' in props.index else props\n",
        "\n",
        "    # Garantir que ambos têm o mesmo índice\n",
        "    common_index = crosstab_categories.index.intersection(props_categories.index)\n",
        "    crosstab_categories = crosstab_categories.loc[common_index]\n",
        "    props_categories = props_categories.loc[common_index]\n",
        "\n",
        "    # Combinar contagem e proporções\n",
        "    analysis = pd.DataFrame({\n",
        "        'categoria': common_index,\n",
        "        'total_registros': crosstab_categories['All'].values,\n",
        "        'classe_0_count': crosstab_categories[0].values,\n",
        "        'classe_1_count': crosstab_categories[1].values,\n",
        "        'classe_0_prop': props_categories[0].values,\n",
        "        'classe_1_prop': props_categories[1].values\n",
        "    })\n",
        "\n",
        "    # Identificar categorias com pouca representação\n",
        "    low_representation = analysis[analysis['total_registros'] < min_samples]\n",
        "\n",
        "    # Identificar categorias com classes desbalanceadas\n",
        "    imbalanced = analysis[\n",
        "        (analysis['classe_0_prop'] < 10) | (analysis['classe_0_prop'] > 90)\n",
        "    ]\n",
        "\n",
        "    return analysis, low_representation, imbalanced\n",
        "\n",
        "# Análise para cada variável categórica\n",
        "categorical_analysis = {}\n",
        "for col in categorical_vars:  # Todas as variáveis categóricas\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"ANÁLISE DE CLASSES PARA: {col}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    analysis, low_rep, imbalanced = analyze_class_representation(train_analysis_df, col)\n",
        "    categorical_analysis[col] = {\n",
        "        'analysis': analysis,\n",
        "        'low_representation': low_rep,\n",
        "        'imbalanced': imbalanced\n",
        "    }\n",
        "\n",
        "    # Estatísticas gerais (simplificadas)\n",
        "    print(f\"\\nESTATÍSTICAS GERAIS:\")\n",
        "    print(f\"   • Total de categorias: {len(analysis)}\")\n",
        "    print(f\"   • Categorias com < 10 registros: {len(low_rep)}\")\n",
        "    print(f\"   • Categorias desbalanceadas (>90% ou <10% de uma classe): {len(imbalanced)}\")\n",
        "\n",
        "    # Visualização - Top 20 categorias mais frequentes\n",
        "    top_20 = analysis.sort_values('total_registros', ascending=False).head(20)\n",
        "\n",
        "    # Criar gráficos lado a lado\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    # Gráfico 1: Frequência das categorias\n",
        "    ax1 = axes[0]\n",
        "    bars = ax1.bar(range(len(top_20)), top_20['total_registros'], color='skyblue', edgecolor='darkblue')\n",
        "    ax1.set_title(f'{col}: Frequência das Top 20 Categorias', fontsize=14, fontweight='bold')\n",
        "    ax1.set_xlabel('Categorias')\n",
        "    ax1.set_ylabel('Número de Registros')\n",
        "    ax1.set_xticks(range(len(top_20)))\n",
        "    ax1.set_xticklabels([str(x) for x in top_20['categoria']], rotation=45, ha='right')\n",
        "    ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Adicionar valores nas barras\n",
        "    for i, bar in enumerate(bars):\n",
        "        height = bar.get_height()\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
        "                f'{int(height)}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "    # Gráfico 2: Proporção de classes por categoria\n",
        "    ax2 = axes[1]\n",
        "    width = 0.35\n",
        "    x = np.arange(len(top_20))\n",
        "\n",
        "    bars1 = ax2.bar(x - width/2, top_20['classe_0_prop'], width,\n",
        "                   label='Classe 0 (Negado)', color='#FF6B6B', alpha=0.8)\n",
        "    bars2 = ax2.bar(x + width/2, top_20['classe_1_prop'], width,\n",
        "                   label='Classe 1 (Permitido)', color='#4ECDC4', alpha=0.8)\n",
        "\n",
        "    ax2.set_title(f'{col}: Proporção de Classes por Categoria', fontsize=14, fontweight='bold')\n",
        "    ax2.set_xlabel('Categorias')\n",
        "    ax2.set_ylabel('Proporção (%)')\n",
        "    ax2.set_xticks(x)\n",
        "    ax2.set_xticklabels([str(x) for x in top_20['categoria']], rotation=45, ha='right')\n",
        "    ax2.legend()\n",
        "    ax2.set_ylim(0, 100)\n",
        "    ax2.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Adicionar linha de referência em 50%\n",
        "    ax2.axhline(y=50, color='black', linestyle='--', alpha=0.5, label='Balanceamento (50%)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Criar resumo consolidado\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"RESUMO CONSOLIDADO\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "summary_data = []\n",
        "for var, data in categorical_analysis.items():\n",
        "    analysis = data['analysis']\n",
        "    low_rep = data['low_representation']\n",
        "    imbalanced = data['imbalanced']\n",
        "\n",
        "    summary_data.append({\n",
        "        'variavel': var,\n",
        "        'total_categorias': len(analysis),\n",
        "        'categorias_pouca_repr': len(low_rep),\n",
        "        'perc_pouca_repr': (len(low_rep) / len(analysis)) * 100,\n",
        "        'registros_pouca_repr': low_rep['total_registros'].sum(),\n",
        "        'perc_registros_pouca_repr': (low_rep['total_registros'].sum() / len(train_df)) * 100,\n",
        "        'categorias_desbalanceadas': len(imbalanced),\n",
        "        'perc_desbalanceadas': (len(imbalanced) / len(analysis)) * 100\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "summary_df = summary_df.sort_values('perc_registros_pouca_repr', ascending=False)\n",
        "\n",
        "print(\"\\nRESUMO POR VARIÁVEL:\")\n",
        "print(f\"{'Variável':<15} {'Total':<8} {'Pouca Repr':<12} {'%Pouca Repr':<12} {'Reg Pouca':<12} {'%Reg Pouca':<12} {'Desbal':<8} {'%Desbal':<8}\")\n",
        "print(\"-\" * 100)\n",
        "for _, row in summary_df.iterrows():\n",
        "    print(f\"{row['variavel']:<15} {row['total_categorias']:<8} \"\n",
        "          f\"{row['categorias_pouca_repr']:<12} {row['perc_pouca_repr']:<12.1f} \"\n",
        "          f\"{row['registros_pouca_repr']:<12} {row['perc_registros_pouca_repr']:<12.2f} \"\n",
        "          f\"{row['categorias_desbalanceadas']:<8} {row['perc_desbalanceadas']:<8.1f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca43fea4",
      "metadata": {
        "id": "ca43fea4"
      },
      "outputs": [],
      "source": [
        "# Padronizar todo o dataset (X)\n",
        "scaler_full = StandardScaler()\n",
        "X_scaled_full = scaler_full.fit_transform(X_train)\n",
        "\n",
        "# Aplicar PCA para 2 componentes em todo o dataset\n",
        "pca_full = PCA(n_components=2, random_state=42)\n",
        "X_pca_full_2d = pca_full.fit_transform(X_scaled_full)\n",
        "\n",
        "# Plot PCA de todo o dataset\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X_pca_full_2d[:, 0], X_pca_full_2d[:, 1], c=y_train, cmap='coolwarm', alpha=0.5)\n",
        "plt.title('PCA (2D) de Todo o Dataset')\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.colorbar(label='ACTION')\n",
        "plt.show()\n",
        "explained_variance = pca_full.explained_variance_ratio_\n",
        "\n",
        "print(f\"Variância explicada por PC1: {explained_variance[0]:.2%}\")\n",
        "print(f\"Variância explicada por PC2: {explained_variance[1]:.2%}\")\n",
        "print(f\"Variância total explicada pelos 2 componentes: {explained_variance.sum():.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f519e70",
      "metadata": {
        "id": "5f519e70"
      },
      "outputs": [],
      "source": [
        "# Treinar o modelo no conjunto de treino original\n",
        "rf_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Obter importâncias das features\n",
        "importances = rf_model.feature_importances_\n",
        "feature_names = X_train.columns\n",
        "indices = importances.argsort()[::-1]\n",
        "\n",
        "# Plotar gráfico de importância das features\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.title(\"Importância das Features - RandomForest\", fontsize=16, fontweight='bold')\n",
        "bars = plt.bar(range(len(importances)), importances[indices], color='skyblue', edgecolor='darkblue')\n",
        "plt.xticks(range(len(importances)), feature_names[indices], rotation=45, ha='right')\n",
        "plt.xlabel(\"Features\", fontsize=12)\n",
        "plt.ylabel(\"Importância\", fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CRISP-DM Fase 3: Preparação dos dados"
      ],
      "metadata": {
        "id": "c1cVjklmRbnf"
      },
      "id": "c1cVjklmRbnf"
    },
    {
      "cell_type": "markdown",
      "id": "799d2134",
      "metadata": {
        "id": "799d2134"
      },
      "source": [
        "## 4.0 Pré-processamento"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Remoção de colunas"
      ],
      "metadata": {
        "id": "ieVxLiemRweg"
      },
      "id": "ieVxLiemRweg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24b37dcf",
      "metadata": {
        "id": "24b37dcf"
      },
      "outputs": [],
      "source": [
        "# Remover as colunas indesejadas\n",
        "cols_to_remove = ['ROLE_ROLLUP_1', 'ROLE_FAMILY']\n",
        "X_train_clean = X_train.drop(columns=cols_to_remove, errors='ignore')\n",
        "X_test_clean = X_test.drop(columns=cols_to_remove, errors='ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Alteração dos tipos de dados"
      ],
      "metadata": {
        "id": "edJMHL5cRzOm"
      },
      "id": "edJMHL5cRzOm"
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista de colunas que são categorias, mas estão com tipo numérico\n",
        "categorical_int_cols = [\n",
        "    'MGR_ID',\n",
        "    'ROLE_ROLLUP_2',\n",
        "    'ROLE_DEPTNAME',\n",
        "    'ROLE_TITLE',\n",
        "    'ROLE_FAMILY_DESC',\n",
        "    'ROLE_CODE',\n",
        "    'RESOURCE'\n",
        "]\n",
        "# Loop para converter as colunas para o tipo 'object'\n",
        "for col in categorical_int_cols:\n",
        "    X_train_clean[col] = X_train_clean[col].astype(str) # Ou .astype('object')\n",
        "    X_test_clean[col] = X_test_clean[col].astype(str) # o mesmo no teste!\n",
        "\n",
        "# Verificando a mudança\n",
        "print(\"Novos tipos de dados em X_train_clean:\")\n",
        "print(X_train_clean[categorical_int_cols].info())"
      ],
      "metadata": {
        "id": "xjPFy1mqRvMW"
      },
      "id": "xjPFy1mqRvMW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Codificação (Encoding)"
      ],
      "metadata": {
        "id": "4ZheBqi_SAdQ"
      },
      "id": "4ZheBqi_SAdQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87d64550",
      "metadata": {
        "id": "87d64550"
      },
      "outputs": [],
      "source": [
        "low_cardinality_features = [col for col in X_train_clean.columns if X_train_clean[col].nunique() < 450]\n",
        "high_cardinality_features = [col for col in X_train_clean.columns if X_train_clean[col].nunique() >= 450]\n",
        "\n",
        "print(low_cardinality_features, high_cardinality_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b60e9b9",
      "metadata": {
        "id": "3b60e9b9"
      },
      "outputs": [],
      "source": [
        "low_cardinality_features = [col for col in X_train_clean.columns if X_train_clean[col].nunique() < 450]\n",
        "high_cardinality_features = [col for col in X_train_clean.columns if X_train_clean[col].nunique() >= 450]\n",
        "\n",
        "# Pré-processador para o outros modelos\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('low_card', OneHotEncoder(handle_unknown='ignore', sparse_output=False, drop='if_binary', min_frequency=0.01), low_cardinality_features),\n",
        "    ('high_card', TargetEncoder(smoothing=1.0, min_samples_leaf=20), high_cardinality_features)\n",
        "], remainder='passthrough')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c197e1fa",
      "metadata": {
        "id": "c197e1fa"
      },
      "outputs": [],
      "source": [
        "# dimensões finais dos dados transformados\n",
        "X_transformed = preprocessor.fit_transform(X_train_clean, y_train)\n",
        "print(\"Dimensões finais dos dados após o pré-processamento:\", X_transformed.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4 Criação do Pipeline (preprocessor + classifier)"
      ],
      "metadata": {
        "id": "LgkUrFLySTJv"
      },
      "id": "LgkUrFLySTJv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73e717a7",
      "metadata": {
        "id": "73e717a7"
      },
      "outputs": [],
      "source": [
        "def create_pipeline(classifier):\n",
        "    \"\"\"\n",
        "    Cria um pipeline de machine learning com duas etapas principais:\n",
        "    1. Um pré-processador para transformar os dados.\n",
        "    2. Um classificador para realizar a predição.\n",
        "    \"\"\"\n",
        "    return Pipeline(steps=[\n",
        "        # Etapa 1: Pré-processamento dos dados\n",
        "        ('preprocessor', preprocessor),\n",
        "\n",
        "        # Etapa 2: O classificador\n",
        "        ('classifier', classifier)\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CRISP-DM Fase 4 e 5: Modelagem e Avaliação"
      ],
      "metadata": {
        "id": "2JUyWk7sQe83"
      },
      "id": "2JUyWk7sQe83"
    },
    {
      "cell_type": "markdown",
      "id": "07f2b224",
      "metadata": {
        "id": "07f2b224"
      },
      "source": [
        "## 5.0 Seleção de Modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c79c33f",
      "metadata": {
        "id": "0c79c33f"
      },
      "outputs": [],
      "source": [
        "models = {\n",
        "    \"K-NN\": KNeighborsClassifier(),\n",
        "    \"Árvore de Decisão\": DecisionTreeClassifier(random_state=42),\n",
        "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
        "    \"SVM (Kernel RBF)\": SVC(random_state=42, max_iter=1000, probability=True)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a6dd8c2",
      "metadata": {
        "id": "1a6dd8c2"
      },
      "outputs": [],
      "source": [
        "# Dicionário de métricas que queremos calcular\n",
        "scoring = {\n",
        "    'roc_auc': 'roc_auc',\n",
        "    'precision_0': make_scorer(precision_score, pos_label=0, zero_division=0),\n",
        "    'precision_1': make_scorer(precision_score, pos_label=1, zero_division=0),\n",
        "    'recall_0': make_scorer(recall_score, pos_label=0, zero_division=0),\n",
        "    'recall_1': make_scorer(recall_score, pos_label=1, zero_division=0)\n",
        "}\n",
        "\n",
        "# Estratégia de validação cruzada estratificada\n",
        "cv_strategy = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# Dicionário para armazenar os resultados finais\n",
        "results = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7666788c",
      "metadata": {
        "id": "7666788c"
      },
      "outputs": [],
      "source": [
        "for name, model in models.items():\n",
        "    print(f\"Avaliando: {name}...\")\n",
        "    pipeline = create_pipeline(model) # Usa o seu preprocessor principal\n",
        "\n",
        "    # Executa a validação cruzada com múltiplas métricas\n",
        "    scores = cross_validate(\n",
        "        pipeline, X_train_clean, y_train,\n",
        "        cv=cv_strategy, scoring=scoring, n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # Armazena a média de cada métrica no dicionário de resultados\n",
        "    results[name] = {\n",
        "        'roc_auc_media': scores['test_roc_auc'].mean(),\n",
        "        'roc_auc_std': scores['test_roc_auc'].std(),\n",
        "        'precision_0_media': scores['test_precision_0'].mean(),\n",
        "        'precision_1_media': scores['test_precision_1'].mean(),\n",
        "        'recall_0_media': scores['test_recall_0'].mean(),\n",
        "        'recall_1_media': scores['test_recall_1'].mean()\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79a91f99",
      "metadata": {
        "id": "79a91f99"
      },
      "outputs": [],
      "source": [
        "# Implementação e avaliação do modelo LVQ\n",
        "print(\"=== AVALIAÇÃO DO MODELO LVQ ===\")\n",
        "\n",
        "# Preparar dados para LVQ (formato de lista)\n",
        "def prepare_data_for_lvq(X, y):\n",
        "    \"\"\"Converte dados para formato esperado pelo LVQ\"\"\"\n",
        "    data = []\n",
        "    for i in range(len(X)):\n",
        "        row = list(X.iloc[i].values) + [y.iloc[i]]\n",
        "        data.append(row)\n",
        "    return data\n",
        "\n",
        "# Converter dados de treino para formato LVQ (usar apenas dados numéricos)\n",
        "lvq_train_data = prepare_data_for_lvq(X_train, y_train)\n",
        "\n",
        "# Avaliação com validação cruzada\n",
        "print(\"Executando validação cruzada com LVQ...\")\n",
        "\n",
        "# Parâmetros do LVQ\n",
        "n_codebooks = 20\n",
        "learning_rate = 0.3\n",
        "epochs = 100\n",
        "n_folds = 5\n",
        "\n",
        "# Executar validação cruzada\n",
        "lvq_scores = evaluate_algorithm(lvq_train_data, learning_vector_quantization,\n",
        "                               n_folds, n_codebooks, learning_rate, epochs)\n",
        "\n",
        "# Calcular métricas\n",
        "lvq_mean_accuracy = np.mean(lvq_scores)\n",
        "lvq_std_accuracy = np.std(lvq_scores)\n",
        "\n",
        "print(f\"\\nResultados da Validação Cruzada LVQ:\")\n",
        "print(f\"Acurácia média: {lvq_mean_accuracy:.3f}%\")\n",
        "print(f\"Desvio padrão: {lvq_std_accuracy:.3f}%\")\n",
        "print(f\"Scores por fold: {[f'{score:.2f}%' for score in lvq_scores]}\")\n",
        "\n",
        "# Adicionar LVQ aos resultados\n",
        "# Calculate precision metrics for LVQ from cross-validation results\n",
        "def calculate_lvq_precision_metrics(lvq_train_data, n_folds, n_codebooks, learning_rate, epochs):\n",
        "    \"\"\"Calculate precision for both classes using cross-validation\"\"\"\n",
        "    folds = cross_validation_split(lvq_train_data, n_folds)\n",
        "    precisions_0 = []\n",
        "    precisions_1 = []\n",
        "\n",
        "    for fold in folds:\n",
        "        train_set = list(folds)\n",
        "        train_set.remove(fold)\n",
        "        train_set = sum(train_set, [])\n",
        "        test_set = list()\n",
        "        for row in fold:\n",
        "            row_copy = list(row)\n",
        "            test_set.append(row_copy)\n",
        "            row_copy[-1] = None\n",
        "\n",
        "        predicted = learning_vector_quantization(train_set, test_set, n_codebooks, learning_rate, epochs)\n",
        "        actual = [row[-1] for row in fold]\n",
        "\n",
        "        # Calculate precision for each class\n",
        "        tp_0 = sum(1 for a, p in zip(actual, predicted) if a == 0 and p == 0)\n",
        "        fp_0 = sum(1 for a, p in zip(actual, predicted) if a == 1 and p == 0)\n",
        "        tp_1 = sum(1 for a, p in zip(actual, predicted) if a == 1 and p == 1)\n",
        "        fp_1 = sum(1 for a, p in zip(actual, predicted) if a == 0 and p == 1)\n",
        "\n",
        "        precision_0 = tp_0 / (tp_0 + fp_0) if (tp_0 + fp_0) > 0 else 0.0\n",
        "        precision_1 = tp_1 / (tp_1 + fp_1) if (tp_1 + fp_1) > 0 else 0.0\n",
        "\n",
        "        precisions_0.append(precision_0)\n",
        "        precisions_1.append(precision_1)\n",
        "\n",
        "    return np.mean(precisions_0), np.mean(precisions_1)\n",
        "\n",
        "# Calculate precision metrics\n",
        "lvq_mean_precision_0, lvq_mean_precision_1 = calculate_lvq_precision_metrics(\n",
        "    lvq_train_data, n_folds, n_codebooks, learning_rate, epochs\n",
        ")\n",
        "\n",
        "results[\"LVQ\"] = {\n",
        "    'roc_auc_media': lvq_mean_accuracy / 100.0,  # Usar acurácia como proxy para ROC-AUC\n",
        "    'roc_auc_std': lvq_std_accuracy / 100.0,\n",
        "    'precision_0_media': lvq_mean_precision_0,\n",
        "    'precision_1_media': lvq_mean_precision_1\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa2f402e",
      "metadata": {
        "id": "fa2f402e"
      },
      "outputs": [],
      "source": [
        "results_df = pd.DataFrame(results).T\n",
        "results_df = results_df.sort_values(by='roc_auc_media', ascending=False)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"             RESULTADO FINAL DA AVALIAÇÃO BASE\")\n",
        "print(\"=\" * 60)\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef9d943a",
      "metadata": {
        "id": "ef9d943a"
      },
      "source": [
        "## 6.0 Busca de Hiperparâmetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb54cd8c",
      "metadata": {
        "id": "cb54cd8c"
      },
      "outputs": [],
      "source": [
        "# --- Dicionário com as Distribuições de Hiperparâmetros ---\n",
        "param_distributions = {\n",
        "    \"K-NN\": {\n",
        "        'classifier__n_neighbors': randint(3, 30),\n",
        "        'classifier__weights': ['uniform', 'distance'],\n",
        "        'classifier__metric': ['euclidean', 'manhattan']\n",
        "    },\n",
        "    \"Árvore de Decisão\": {\n",
        "        'classifier__max_depth': randint(3, 21),\n",
        "        'classifier__min_samples_split': randint(2, 11),\n",
        "        'classifier__min_samples_leaf': randint(1, 11),\n",
        "        'classifier__criterion': ['gini', 'entropy'],\n",
        "        'classifier__class_weight': ['balanced', None]\n",
        "    },\n",
        "    \"Random Forest\": {\n",
        "        'classifier__n_estimators': randint(50, 301),\n",
        "        'classifier__max_depth': randint(3, 21),\n",
        "        'classifier__min_samples_split': randint(2, 11),\n",
        "        'classifier__min_samples_leaf': randint(1, 11),\n",
        "        'classifier__max_features': ['sqrt', 'log2', None],\n",
        "        'classifier__class_weight': ['balanced', None],\n",
        "        'classifier__bootstrap': [True, False]\n",
        "    },\n",
        "    \"SVM (Kernel RBF)\": {\n",
        "        'classifier__C': loguniform(0.1, 100),\n",
        "        'classifier__gamma': loguniform(0.001, 1),\n",
        "        'classifier__kernel': ['rbf'],\n",
        "        'classifier__class_weight': ['balanced', None]\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "136e081a",
      "metadata": {
        "id": "136e081a"
      },
      "outputs": [],
      "source": [
        "# Dicionário para armazenar os resultados da busca\n",
        "search_results = {}\n",
        "\n",
        "# Loop principal para otimizar cada modelo\n",
        "for name, model in models.items():\n",
        "    # Pula modelos que não estão no nosso dicionário de busca (como o LVQ)\n",
        "    if name not in param_distributions:\n",
        "        continue\n",
        "\n",
        "    print(f\"--> Otimizando o modelo: {name}\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Cria o pipeline com o pré-processador + classificador\n",
        "    pipeline = create_pipeline(model)\n",
        "\n",
        "    # Configura a busca aleatória de hiperparâmetros\n",
        "    random_search = RandomizedSearchCV(\n",
        "        estimator=pipeline,\n",
        "        param_distributions=param_distributions[name],\n",
        "        n_iter=30,          # Número de combinações a testar\n",
        "        cv=cv_strategy,     # Usando a validação cruzada estratificada\n",
        "        scoring='roc_auc',  # Focando na métrica roc auc\n",
        "        n_jobs=-1,          # Utiliza todos os processadores\n",
        "        random_state=42,\n",
        "        verbose=0           # Mantido em 0 para uma saída limpa\n",
        "    )\n",
        "\n",
        "    # Executa a busca com os dados de treino\n",
        "    random_search.fit(X_train_clean, y_train)\n",
        "\n",
        "    # Calcula o tempo de duração da busca\n",
        "    duration = time.time() - start_time\n",
        "\n",
        "    # Armazena os resultados mais importantes\n",
        "    search_results[name] = {\n",
        "        'best_score': random_search.best_score_,\n",
        "        'best_params': random_search.best_params_,\n",
        "        'best_estimator': random_search.best_estimator_\n",
        "    }\n",
        "\n",
        "    # Resumo claro dos resultados para este modelo\n",
        "    print(f\"    Melhor ROC AUC (CV): {random_search.best_score_:.4f}\")\n",
        "    print(f\"    Melhores Parâmetros: {random_search.best_params_}\")\n",
        "    print(f\"    Duração da busca: {duration:.2f} segundos\")\n",
        "    print(\"-\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8857ffd",
      "metadata": {
        "id": "b8857ffd"
      },
      "outputs": [],
      "source": [
        "# Extrai os melhores modelos da busca\n",
        "best_models = {name: result['best_estimator'] for name, result in search_results.items()}\n",
        "\n",
        "# Configura a figura\n",
        "plt.figure(figsize=(12, 9))\n",
        "ax = plt.gca()\n",
        "\n",
        "# Itera sobre os melhores modelos para plotar cada curva\n",
        "for name, model_pipeline in best_models.items():\n",
        "    RocCurveDisplay.from_estimator(\n",
        "    model_pipeline, X_test_clean, y_test, name=name, ax=ax, curve_kwargs={'alpha': 0.8}\n",
        ")\n",
        "\n",
        "# Adiciona a linha de referência (classificador aleatório)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Aleatório (AUC = 0.5)')\n",
        "\n",
        "# Customiza o gráfico\n",
        "plt.title('Curva ROC Comparativa dos Modelos Otimizados', fontsize=16)\n",
        "plt.xlabel('Taxa de Falsos Positivos', fontsize=12)\n",
        "plt.ylabel('Taxa de Verdadeiros Positivos (Recall)', fontsize=12)\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}